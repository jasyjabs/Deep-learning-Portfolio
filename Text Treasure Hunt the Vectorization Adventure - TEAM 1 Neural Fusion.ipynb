{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3gJ6nOQD3hP"
   },
   "source": [
    "### Team1 - Neural Fusion\n",
    "\n",
    "\n",
    "### Introduction \n",
    "\n",
    "Objective:\n",
    "\n",
    "To solve a series of challenges, each leading closer to uncovering the final treasure. \n",
    "\n",
    "Along the way, we are to employ text processing techniques to decode hidden clues embedded within texts,to decode clues, uncover hidden connections, and collaborate with others to reach the ultimate treasure! \n",
    "\n",
    "Clues for Group 1\n",
    "1. Clue 1a (Easier): From lines and shapes to scenes so bright, I give digital creations the illusion of light.\n",
    "2. Clue 2a (Harder): Where polygons dance and shaders weave, my artistry helps the virtual world breathe.\n",
    "3. Clue 1 b(Easier): Tiny squares, a canvas wide, building images side by side.\n",
    "4. Clue 2b (Harder): RGB spells my secret code, the building blocks where colors explode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the necessary libraries - Setting Up \n",
    "\n",
    "#ensuring we have all the necessary libraries installed and importing them into our Python environment\n",
    "\n",
    "#The libraries include : NLTK, Pandas, Scikit-learn, Gensim, and Spacy. Additionally, we provide an optional section for advanced exploration with Transformers, which requires installation and importation of the Transformers library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\fisay\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: torch in c:\\users\\fisay\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Collecting torch\n",
      "  Downloading torch-2.3.0-cp39-cp39-win_amd64.whl (159.7 MB)\n",
      "     -------------------------------------- 159.7/159.7 MB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: transformers in c:\\users\\fisay\\anaconda3\\lib\\site-packages (4.39.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "     ---------------------------------------- 9.0/9.0 MB 36.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1\n",
      "  Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "     -------------------------------------- 228.5/228.5 MB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sympy in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: requests in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 20.1 MB/s eta 0:00:00\n",
      "Collecting intel-openmp==2021.*\n",
      "  Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 37.3 MB/s eta 0:00:00\n",
      "Collecting tbb==2021.*\n",
      "  Downloading tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "     ------------------------------------- 286.4/286.4 kB 17.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: tbb, intel-openmp, mkl, torch, tokenizers, transformers\n",
      "  Attempting uninstall: tbb\n",
      "    Found existing installation: TBB 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Matplotlib in c:\\users\\fisay\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from Matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from Matplotlib) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from Matplotlib) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from Matplotlib) (1.24.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from Matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from Matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from Matplotlib) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from Matplotlib) (9.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->Matplotlib) (1.16.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\fisay\\anaconda3\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from seaborn) (1.4.4)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from seaborn) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from seaborn) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fisay\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install torch\n",
    "!pip install --upgrade torch transformers\n",
    "!pip install Matplotlib\n",
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "McRTSlMnBCh_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fisay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\fisay\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "#import system\n",
    "\n",
    "import matplotlib.pyplot as plt #data viz\n",
    "import seaborn as sns # data viz\n",
    "#supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec  # For Word2Vec embeddings\n",
    "import re  # For regular expressions\n",
    "\n",
    "# Optional advanced exploration with Transformers\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvGnUmbOGVxC"
   },
   "source": [
    "# Quest Begins â€“ The Initial Clue\n",
    "*Clue 1a (Easier): From lines and shapes to scenes so bright, I give digital creations the illusion of light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wFQiI91mGytN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1187, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the 20 newsgroups dataset for 'sci.med' and 'sci.space' categories\n",
    "\n",
    "categories = [\"sci.med\", \"sci.space\"]\n",
    "newsgroups_train = fetch_20newsgroups(subset=\"train\", categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset=\"test\", categories=categories)\n",
    "\n",
    "# Create a dataframe for 'sci.med' and 'sci.space' categories\n",
    "df = pd.DataFrame(data=newsgroups_train.data, columns=[\"text\"])\n",
    "df[\"target\"] = newsgroups_train.target\n",
    "df[\"category\"] = df[\"target\"].map(lambda x: categories[x])\n",
    "\n",
    "# Display the number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: flb@flb.optiplan.fi (\"F.Baube[tm]\")\\nSub...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: geb@cs.pitt.edu (Gordon Banks)\\nSubject:...</td>\n",
       "      <td>0</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: ab961@Freenet.carleton.ca (Robert Alliso...</td>\n",
       "      <td>0</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: rind@enterprise.bih.harvard.edu (David R...</td>\n",
       "      <td>0</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: nsmca@aurora.alaska.edu\\nSubject: Space ...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target   category\n",
       "0  From: flb@flb.optiplan.fi (\"F.Baube[tm]\")\\nSub...       1  sci.space\n",
       "1  From: geb@cs.pitt.edu (Gordon Banks)\\nSubject:...       0    sci.med\n",
       "2  From: ab961@Freenet.carleton.ca (Robert Alliso...       0    sci.med\n",
       "3  From: rind@enterprise.bih.harvard.edu (David R...       0    sci.med\n",
       "4  From: nsmca@aurora.alaska.edu\\nSubject: Space ...       1  sci.space"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows to test\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>From: clarke@acme.ucf.edu (Thomas Clarke)\\nSub...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>From: bruce@Data-IO.COM (Bruce Reynolds)\\nSubj...</td>\n",
       "      <td>0</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>From: baalke@kelvin.jpl.nasa.gov (Ron Baalke)\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>From: 18084TM@msu.edu (Tom)\\nSubject: Level 5?...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>From: steveo@world.std.com (Steven W Orr)\\nSub...</td>\n",
       "      <td>0</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target   category\n",
       "1182  From: clarke@acme.ucf.edu (Thomas Clarke)\\nSub...       1  sci.space\n",
       "1183  From: bruce@Data-IO.COM (Bruce Reynolds)\\nSubj...       0    sci.med\n",
       "1184  From: baalke@kelvin.jpl.nasa.gov (Ron Baalke)\\...       1  sci.space\n",
       "1185  From: 18084TM@msu.edu (Tom)\\nSubject: Level 5?...       1  sci.space\n",
       "1186  From: steveo@world.std.com (Steven W Orr)\\nSub...       0    sci.med"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1187 entries, 0 to 1186\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      1187 non-null   object\n",
      " 1   target    1187 non-null   int64 \n",
      " 2   category  1187 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 27.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>1187.0</td>\n",
       "      <td>0.499579</td>\n",
       "      <td>0.500211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count      mean       std  min  25%  50%  75%  max\n",
       "target  1187.0  0.499579  0.500211  0.0  0.0  0.0  1.0  1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCxTucp7GzWv"
   },
   "source": [
    "# Keyword Quest\n",
    "# Starting with TF-IDF:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clue: From lines and shapes to scenes so bright, I give digital creations the illusion of light.\n",
      "Relevant keywords: ['and', 'art', 'artistic']\n",
      "\n",
      "Clue: Where polygons dance and shaders weave, my artistry helps the virtual world breathe.\n",
      "Relevant keywords: ['artistic', 'art', 'and']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def extract_keywords(text_data, clues, top_n=10):\n",
    "    # Preprocess the text data\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preprocessed_data = [' '.join([word for word in text.lower().split() if word not in stop_words]) for text in text_data]\n",
    "\n",
    "    # Create a TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(preprocessed_data)\n",
    "\n",
    "    # Get the feature names (keywords)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Preprocess the clues\n",
    "    preprocessed_clues = [' '.join([word for word in clue.lower().split() if word not in stop_words]) for clue in clues]\n",
    "\n",
    "    # Transform the clues using the same vectorizer\n",
    "    clue_vectors = vectorizer.transform(preprocessed_clues)\n",
    "\n",
    "    # Calculate cosine similarity between clues and text data\n",
    "    similarity_scores = cosine_similarity(clue_vectors, tfidf_matrix)\n",
    "\n",
    "    # Find the top N most similar keywords for each clue\n",
    "    top_keywords = []\n",
    "    for i in range(len(clues)):\n",
    "        clue_scores = similarity_scores[i]\n",
    "        top_indices = clue_scores.argsort()[-top_n:][::-1]\n",
    "        top_keywords.append([feature_names[index] for index in top_indices])\n",
    "\n",
    "    return top_keywords\n",
    "\n",
    "clues = [\n",
    "    \"From lines and shapes to scenes so bright, I give digital creations the illusion of light.\",\n",
    "    \"Where polygons dance and shaders weave, my artistry helps the virtual world breathe.\"\n",
    "]\n",
    "\n",
    "text_data = [\n",
    "    \"Digital art encompasses a range of artistic work and practices that use digital technology as an essential part of the creative and/or presentation process.\",\n",
    "    \"It involves creating artwork digitally, either by computer software or other digital tools.\",\n",
    "    \"The resulting artwork can be printed, displayed on a screen, or projected onto a surface.\"\n",
    "]\n",
    "\n",
    "# Extract relevant keywords based on the clues\n",
    "relevant_keywords = extract_keywords(text_data, clues)\n",
    "\n",
    "# Print the relevant keywords for each clue\n",
    "for i, clue in enumerate(clues):\n",
    "    print(f\"Clue: {clue}\")\n",
    "    print(\"Relevant keywords:\", relevant_keywords[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci Med Clues: ['from lines and shapes to scenes so bright i give digital creations the illusion of light', 'where polygons dance and shaders weave my artistry helps the virtual world breathe', 'tiny squares a canvas wide building images side by side', 'rgb spells my secret code the building blocks where colors explode']\n",
      "----------------\n",
      "Sci Space Clues: ['from lines and shapes to scenes so bright i give digital creations the illusion of light', 'where polygons dance and shaders weave my artistry helps the virtual world breathe', 'tiny squares a canvas wide building images side by side', 'rgb spells my secret code the building blocks where colors explode']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Clues\n",
    "scimed_Clue_1a = (\n",
    "    \"From lines and shapes to scenes so bright, I give digital creations the illusion of light.\"\n",
    ")\n",
    "scimed_Clue_2a = (\n",
    "    \"Where polygons dance and shaders weave, my artistry helps the virtual world breathe.\"\n",
    ")\n",
    "scimed_Clue_1b = (\n",
    "    \"Tiny squares, a canvas wide, building images side by side.\"\n",
    ")\n",
    "scimed_Clue_2b = (\n",
    "    \"RGB spells my secret code, the building blocks where colors explode.\"\n",
    ")\n",
    "\n",
    "\n",
    "# Sci Space topic\n",
    "scispace_Clue_1a = \"From lines and shapes to scenes so bright, I give digital creations the illusion of light.\"\n",
    "scispace_Clue_2a = \"Where polygons dance and shaders weave, my artistry helps the virtual world breathe.\"\n",
    "scispace_Clue_1b = (\n",
    "    \"Tiny squares, a canvas wide, building images side by side.\"\n",
    ")\n",
    "scispace_Clue_2b = \"RGB spells my secret code, the building blocks where colors explode.\"\n",
    "\n",
    "scimed_clues_list = [\n",
    "    scimed_Clue_1a,\n",
    "    scimed_Clue_2a,\n",
    "    scimed_Clue_1b,\n",
    "    scimed_Clue_2b,\n",
    "    \n",
    "]\n",
    "scispace_clues_list = [\n",
    "    scispace_Clue_1a,\n",
    "    scispace_Clue_2a,\n",
    "    scispace_Clue_1b,\n",
    "    scispace_Clue_2b,\n",
    "]\n",
    "\n",
    "# clean the clues list text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "scimed_clues_list = [clean_text(clue) for clue in scimed_clues_list]\n",
    "scispace_clues_list = [clean_text(clue) for clue in scispace_clues_list]\n",
    "\n",
    "print(f\"Sci Med Clues: {scimed_clues_list}\")\n",
    "print(\"----------------\")\n",
    "print(f\"Sci Space Clues: {scispace_clues_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings like Word2Vec or GloVe help us understand how words relate to each other.\n",
    "\n",
    "# Topkey & Related  words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "towQxMlhG92o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top keywords for 'sci.med' clues: ['bright' 'artistry' 'canvas' 'blocks']\n",
      "----------------\n",
      "Top keywords for 'sci.space' clues: ['bright' 'artistry' 'canvas' 'blocks']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Clues\n",
    "scimed_Clue_1a = (\n",
    "    \"From lines and shapes to scenes so bright, I give digital creations the illusion of light.\"\n",
    ")\n",
    "scimed_Clue_2a = (\n",
    "    \"Where polygons dance and shaders weave, my artistry helps the virtual world breathe.\"\n",
    ")\n",
    "scimed_Clue_1b = (\n",
    "    \"Tiny squares, a canvas wide, building images side by side.\"\n",
    ")\n",
    "scimed_Clue_2b = (\n",
    "    \"RGB spells my secret code, the building blocks where colors explode.\"\n",
    ")\n",
    "\n",
    "\n",
    "# Sci Space topic\n",
    "scispace_Clue_1a = \"From lines and shapes to scenes so bright, I give digital creations the illusion of light.\"\n",
    "scispace_Clue_2a = \"Where polygons dance and shaders weave, my artistry helps the virtual world breathe.\"\n",
    "scispace_Clue_1b = (\n",
    "    \"Tiny squares, a canvas wide, building images side by side.\"\n",
    ")\n",
    "scispace_Clue_2b = \"RGB spells my secret code, the building blocks where colors explode.\"\n",
    "\n",
    "scimed_clues_list = [\n",
    "    scimed_Clue_1a,\n",
    "    scimed_Clue_2a,\n",
    "    scimed_Clue_1b,\n",
    "    scimed_Clue_2b,\n",
    "    \n",
    "]\n",
    "scispace_clues_list = [\n",
    "    scispace_Clue_1a,\n",
    "    scispace_Clue_2a,\n",
    "    scispace_Clue_1b,\n",
    "    scispace_Clue_2b,\n",
    "]\n",
    "\n",
    "# clean the clues list text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "scimed_clues_list = [clean_text(clue) for clue in scimed_clues_list]\n",
    "scispace_clues_list = [clean_text(clue) for clue in scispace_clues_list]\n",
    "\n",
    "def tfidf_extract_keywords(clues_list):\n",
    "    # Create a TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "    # Fit the vectorizer to the clues\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(clues_list)\n",
    "\n",
    "    # Get the feature names of `tfidf_vectorizer`\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a DataFrame of the `tfidf_matrix`\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "    # Get the top feature from each clue\n",
    "    top_keywords = tfidf_df.idxmax(axis=1).values\n",
    "    return top_keywords\n",
    "\n",
    "scimed_related_keywords_tfidf = tfidf_extract_keywords(scimed_clues_list)\n",
    "scispace_related_keywords_tfidf = tfidf_extract_keywords(scispace_clues_list)\n",
    "\n",
    "print(f\"Top keywords for 'sci.med' clues: {scimed_related_keywords_tfidf}\")\n",
    "print(\"----------------\")\n",
    "print(f\"Top keywords for 'sci.space' clues: {scispace_related_keywords_tfidf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related keywords for 'sci.med' clues (GloVe): ['bright' 'artistry' 'canvas' 'blocks']\n",
      "----------------\n",
      "Related keywords for 'sci.space' clues (GloVe): ['bright' 'artistry' 'canvas' 'blocks']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove_file = \"20newsgroups.txt\"\n",
    "glove_embeddings = KeyedVectors.load_word2vec_format(\n",
    "    glove_file, binary=False, no_header=True\n",
    ")\n",
    "\n",
    "\n",
    "def tfidf_extract_keywords(clues_list):\n",
    "    # Create a TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "    # Fit the vectorizer to the clues\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(clues_list)\n",
    "\n",
    "    # Get the feature names of `tfidf_vectorizer`\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a DataFrame of the `tfidf_matrix`\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "    # Get the top feature from each clue\n",
    "    top_keywords = tfidf_df.idxmax(axis=1).values\n",
    "    return top_keywords\n",
    "\n",
    "# Make sure to define scimed_clues_list and scispace_clues_list with correct clues before calling tfidf_extract_keywords function\n",
    "scimed_related_keywords_glove = tfidf_extract_keywords(scimed_clues_list)\n",
    "scispace_related_keywords_glove = tfidf_extract_keywords(scispace_clues_list)\n",
    "\n",
    "print(f\"Related keywords for 'sci.med' clues (GloVe): {scimed_related_keywords_glove}\")\n",
    "print(\"----------------\")\n",
    "print(f\"Related keywords for 'sci.space' clues (GloVe): {scispace_related_keywords_glove}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIt9nP5VHAP9"
   },
   "source": [
    "## reveal new concepts, or hint at hidden patterns within the data.\n",
    "## Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related keywords for 'sci.med' clues: ['and', 'colors', 'spells', 'code', 'scenes', 'dance', 'digital', 'give', 'breathe', 'shapes', 'digital', 'i', 'polygons', 'side', 'blocks', 'explode', 'images', 'my', 'a', 'virtual', 'creations', 'by', 'side', 'where', 'artistry']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Function to get related keywords using Word2Vec\n",
    "def get_related_keywords(clues_list, keywords, n=5):\n",
    "    # Tokenize the clues\n",
    "    tokenized_clues = [clue.split() for clue in clues_list]\n",
    "\n",
    "    # Create a Word2Vec model\n",
    "    word2vec = Word2Vec(tokenized_clues, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "    related_keywords = []\n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            # Get the most similar keywords for each input keyword\n",
    "            related = word2vec.wv.most_similar(positive=[keyword], topn=n)\n",
    "            related_keywords.extend([w[0] for w in related])\n",
    "        except KeyError:\n",
    "            # Skip keywords not in the vocabulary\n",
    "            pass\n",
    "\n",
    "    return related_keywords\n",
    "\n",
    "\n",
    "# Define scimed_keywords (example)\n",
    "scimed_keywords = ['artistry', 'blocks', 'breathe', 'bright', 'building']\n",
    "\n",
    "# Get related keywords for 'sci.med' clues\n",
    "scimed_related_keywords = get_related_keywords(scimed_clues_list, scimed_keywords)\n",
    "\n",
    "print(f\"Related keywords for 'sci.med' clues: {scimed_related_keywords}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kFaZ8vgHJ9Q"
   },
   "source": [
    "# Semantic Safari\n",
    "\n",
    "## Exploring the World of Meaning: Word embeddings like Word2Vec or GloVe help us understand how words relate to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities between 'sci.med' keywords and 'sci.space' clues: [('artistry', 'and', 0.2120572030544281), ('blocks', 'dance', 0.2164992243051529), ('breathe', 'digital', 0.3488596975803375), ('bright', 'explode', 0.1961306929588318), ('building', 'creations', 0.16702575981616974)]\n",
      "----------------\n",
      "Similarities between 'sci.space' keywords and 'sci.med' clues: []\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Function to calculate similarity between keywords and clues\n",
    "def calculate_similarity(keywords, clues_list):\n",
    "    # Tokenize the clues\n",
    "    tokenized_clues = [clue.split() for clue in clues_list]\n",
    "\n",
    "    # Create a Word2Vec model\n",
    "    word2vec = Word2Vec(tokenized_clues, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "    similarities = []\n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            # Get the most similar clues for each keyword\n",
    "            similar_clues = word2vec.wv.most_similar(positive=[keyword], topn=1)\n",
    "            similarities.append((keyword, similar_clues[0][0], similar_clues[0][1]))\n",
    "        except KeyError:\n",
    "            # Skip keywords not in the vocabulary\n",
    "            pass\n",
    "\n",
    "    return similarities\n",
    "\n",
    "# Define scispace_keywords (example)\n",
    "scispace_keywords = ['space', 'galaxy', 'astronomy', 'cosmos', 'planet']\n",
    "\n",
    "# Calculate similarities between 'sci.med' keywords and 'sci.space' clues, and vice versa\n",
    "scimed_similarities = calculate_similarity(scimed_keywords, scispace_clues_list)\n",
    "scispace_similarities = calculate_similarity(scispace_keywords, scimed_clues_list)\n",
    "\n",
    "print(\n",
    "    f\"Similarities between 'sci.med' keywords and 'sci.space' clues: {scimed_similarities}\"\n",
    ")\n",
    "print(\"----------------\")\n",
    "print(\n",
    "    f\"Similarities between 'sci.space' keywords and 'sci.med' clues: {scispace_similarities}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Trying to Calculate similarities between our keywords and texts in other categories. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duuv_021HvNl"
   },
   "outputs": [],
   "source": [
    "# Calculate the average similarity score for 'sci.med' and 'sci.space' clues\n",
    "scimed_similarity_score = np.mean([score for _, _, score in scimed_similarities])\n",
    "scispace_similarity_score = np.mean([score for _, _, score in scispace_similarities])\n",
    "\n",
    "print(f\"Average similarity score for 'sci.med' clues: {scimed_similarity_score}\")\n",
    "print(f\"Average similarity score for 'sci.space' clues: {scispace_similarity_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# why is the confidence score low? ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo_uW51_KV0Q"
   },
   "source": [
    "# Advanced Exploration: Transformers ** (Optional)\n",
    "\n",
    "Transformer-based models provide even more nuanced semantic understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6xI2ScjULjJB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What influences mood in the brain?\n",
      "Answer: light\n",
      "Score: 0.06852194666862488\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the goal of space exploration?\n",
      "Answer: helps the virtual world breathe tiny squares\n",
      "Score: 0.22464647889137268\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This text seems to be about neurotransmitters and their effects on mood.\n",
      "Classification: NEGATIVE\n",
      "Score: 0.9496363401412964\n",
      "----------------\n",
      "Text: This text seems to be about space exploration and technological advancements.\n",
      "Classification: POSITIVE\n",
      "Score: 0.9836159348487854\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# Function to perform question answering using a pre-trained model\n",
    "def answer_question(question, context):\n",
    "    answerer = pipeline(\"question-answering\")\n",
    "    answer = answerer({\"question\": question, \"context\": context})\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Function to perform text classification using a pre-trained model\n",
    "def classify_text(text):\n",
    "    classifier = pipeline(\"text-classification\")\n",
    "    result = classifier(text)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example usage for question answering\n",
    "scimed_question = \"What influences mood in the brain?\"\n",
    "scimed_context = \" \".join(scimed_clues_list)\n",
    "scimed_answer = answer_question(scimed_question, scimed_context)\n",
    "print(f\"Question: {scimed_question}\")\n",
    "print(f\"Answer: {scimed_answer['answer']}\")\n",
    "print(f\"Score: {scimed_answer['score']}\")\n",
    "print(\"----------------\")\n",
    "\n",
    "scispace_question = \"What is the goal of space exploration?\"\n",
    "scispace_context = \" \".join(scispace_clues_list)\n",
    "scispace_answer = answer_question(scispace_question, scispace_context)\n",
    "print(f\"Question: {scispace_question}\")\n",
    "print(f\"Answer: {scispace_answer['answer']}\")\n",
    "print(f\"Score: {scispace_answer['score']}\")\n",
    "print(\"----------------\")\n",
    "\n",
    "# Example usage for text classification\n",
    "scimed_text = \"This text seems to be about neurotransmitters and their effects on mood.\"\n",
    "scimed_classification = classify_text(scimed_text)\n",
    "print(f\"Text: {scimed_text}\")\n",
    "print(f\"Classification: {scimed_classification[0]['label']}\")\n",
    "print(f\"Score: {scimed_classification[0]['score']}\")\n",
    "print(\"----------------\")\n",
    "\n",
    "scispace_text = (\n",
    "    \"This text seems to be about space exploration and technological advancements.\"\n",
    ")\n",
    "scispace_classification = classify_text(scispace_text)\n",
    "print(f\"Text: {scispace_text}\")\n",
    "print(f\"Classification: {scispace_classification[0]['label']}\")\n",
    "print(f\"Score: {scispace_classification[0]['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What influences mood in the brain?\n",
      "Answer: lines and shapes\n",
      "Score: 2.6440488909429405e-06\n",
      "----------------\n",
      "Question: What is the goal of space exploration?\n",
      "Answer: building blocks where colors explode\n",
      "Score: 1.211475506579518e-07\n",
      "----------------\n",
      "Text: This text seems to be about neurotransmitters and their effects on mood.\n",
      "Classification: Negative\n",
      "Score: 0.8315242528915405\n",
      "----------------\n",
      "Text: This text seems to be about space exploration and technological advancements.\n",
      "Classification: Positive\n",
      "Score: 0.9824037551879883\n"
     ]
    }
   ],
   "source": [
    "# Function to perform question answering using a fine-tuned model\n",
    "def answer_question(question, context):\n",
    "    device = 0 if torch.cuda.is_available() else -1  # Check if GPU is available\n",
    "    answerer = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", device=device)\n",
    "    answer = answerer({\"question\": question, \"context\": context})\n",
    "    return answer\n",
    "\n",
    "# Function to perform text classification using a fine-tuned model\n",
    "def classify_text(text):\n",
    "    device = 0 if torch.cuda.is_available() else -1  # Check if GPU is available\n",
    "    classifier = pipeline(\"sentiment-analysis\", model=\"textattack/bert-base-uncased-SST-2\", device=device)\n",
    "    result = classifier(text)\n",
    "    return result\n",
    "\n",
    "# Updated context for the questions\n",
    "scimed_context = \" \".join(scimed_clues_list)\n",
    "scispace_context = \" \".join(scispace_clues_list)\n",
    "\n",
    "# Example usage for question answering\n",
    "scimed_answer = answer_question(\"What influences mood in the brain?\", scimed_context)\n",
    "print(\"Question: What influences mood in the brain?\")\n",
    "print(f\"Answer: {scimed_answer['answer']}\")\n",
    "print(f\"Score: {scimed_answer['score']}\")\n",
    "print(\"----------------\")\n",
    "\n",
    "scispace_answer = answer_question(\"What is the goal of space exploration?\", scispace_context)\n",
    "print(\"Question: What is the goal of space exploration?\")\n",
    "print(f\"Answer: {scispace_answer['answer']}\")\n",
    "print(f\"Score: {scispace_answer['score']}\")\n",
    "print(\"----------------\")\n",
    "\n",
    "# Example usage for text classification\n",
    "scimed_classification = classify_text(\"This text seems to be about neurotransmitters and their effects on mood.\")\n",
    "print(\"Text: This text seems to be about neurotransmitters and their effects on mood.\")\n",
    "print(f\"Classification: {'Positive' if scimed_classification[0]['label'] == 'LABEL_1' else 'Negative'}\")\n",
    "print(f\"Score: {scimed_classification[0]['score']}\")\n",
    "print(\"----------------\")\n",
    "\n",
    "scispace_classification = classify_text(\"This text seems to be about space exploration and technological advancements.\")\n",
    "print(\"Text: This text seems to be about space exploration and technological advancements.\")\n",
    "print(f\"Classification: {'Positive' if scispace_classification[0]['label'] == 'LABEL_1' else 'Negative'}\")\n",
    "print(f\"Score: {scispace_classification[0]['score']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What influences mood in the brain?\n",
      "Answer: brain chemicals\n",
      "Score: 0.4653284251689911\n",
      "----------------\n",
      "Question: What is the goal of space exploration?\n",
      "Answer: space discoveries\n",
      "Score: 0.6120656132698059\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import warnings\n",
    "\n",
    "# Suppress default model warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Using a pipeline.*\")  # Silence warnings\n",
    "\n",
    "\n",
    "def get_question_answering_model(task_name=\"squad\"):\n",
    "  \"\"\"\n",
    "  Fetches a question answering model based on the specified task.\n",
    "\n",
    "  Args:\n",
    "      task_name (str, optional): The task for which the model is optimized.\n",
    "          Defaults to \"squad\".\n",
    "\n",
    "  Returns:\n",
    "      pipeline: A pipeline object loaded with the chosen question answering model.\n",
    "  \"\"\"\n",
    "  # Consider using model selection libraries or exploring Hugging Face Hub for task-specific models\n",
    "  if task_name == \"squad\":\n",
    "    model_name = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
    "  else:\n",
    "    # Add logic for choosing other question answering models based on task_name\n",
    "    raise ValueError(f\"Unsupported task name: {task_name}\")\n",
    "  return pipeline(\"question-answering\", model=model_name)\n",
    "\n",
    "\n",
    "def get_text_classification_model(task_name=\"sentiment-analysis\"):\n",
    "  \"\"\"\n",
    "  Fetches a text classification model based on the specified task.\n",
    "\n",
    "  Args:\n",
    "      task_name (str, optional): The task for which the model is optimized.\n",
    "          Defaults to \"sentiment-analysis\".\n",
    "\n",
    "  Returns:\n",
    "      pipeline: A pipeline object loaded with the chosen text classification model.\n",
    "  \"\"\"\n",
    "  # Consider using model selection libraries or exploring Hugging Face Hub for task-specific models\n",
    "  if task_name == \"sentiment-analysis\":\n",
    "    model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "  else:\n",
    "    # Add logic for choosing other text classification models based on task_name\n",
    "    raise ValueError(f\"Unsupported task name: {task_name}\")\n",
    "  return pipeline(\"text-classification\", model=model_name)\n",
    "\n",
    "\n",
    "# Function to perform question answering using a pre-trained model\n",
    "def answer_question(question, context, model=None):\n",
    "  \"\"\"\n",
    "  Answers a question using a pre-trained question answering model.\n",
    "\n",
    "  Args:\n",
    "      question (str): The question to be answered.\n",
    "      context (str): The context in which to find the answer.\n",
    "      model (pipeline, optional): A pre-loaded question answering pipeline object.\n",
    "          If not provided, a default model will be fetched based on the task name.\n",
    "\n",
    "  Returns:\n",
    "      dict: A dictionary containing the answer and its score.\n",
    "  \"\"\"\n",
    "  if model is None:\n",
    "    question_answering_model = get_question_answering_model()\n",
    "  else:\n",
    "    question_answering_model = model\n",
    "  answer = question_answering_model({\"question\": question, \"context\": context})\n",
    "  return answer\n",
    "\n",
    "\n",
    "# Function to perform text classification using a pre-trained model\n",
    "def classify_text(text, model=None):\n",
    "  \"\"\"\n",
    "  Classifies text using a pre-trained text classification model.\n",
    "\n",
    "  Args:\n",
    "      text (str): The text to be classified.\n",
    "      model (pipeline, optional): A pre-loaded text classification pipeline object.\n",
    "          If not provided, a default model will be fetched based on the task name.\n",
    "\n",
    "  Returns:\n",
    "      list: A list containing the classification label and its score.\n",
    "  \"\"\"\n",
    "  if model is None:\n",
    "    text_classification_model = get_text_classification_model()\n",
    "  else:\n",
    "    text_classification_model = model\n",
    "  result = text_classification_model(text)\n",
    "  return result\n",
    "\n",
    "\n",
    "# Example usage\n",
    "scimed_question = \"What influences mood in the brain?\"\n",
    "# Replace with your actual list of clues for SciMed context\n",
    "scimed_clues_list = [\"Clue 1 about neurotransmitters\", \"Clue 2 about brain chemicals\"]\n",
    "scimed_context = \" \".join(scimed_clues_list)\n",
    "\n",
    "scispace_question = \"What is the goal of space exploration?\"\n",
    "# Replace with your actual list of clues for SciSpace context\n",
    "scispace_clues_list = [\"Clue 1 about space exploration\", \"Clue 2 about space discoveries\"]\n",
    "scispace_context = \" \".join(scispace_clues_list)\n",
    "\n",
    "scimed_answer = answer_question(scimed_question, scimed_context)\n",
    "print(f\"Question: {scimed_question}\")\n",
    "print(f\"Answer: {scimed_answer['answer']}\")\n",
    "print(f\"Score: {scimed_answer['score']}\")\n",
    "print(\"----------------\")\n",
    "\n",
    "scispace_answer = answer_question(scispace_question, scispace_context)\n",
    "# Print regardless of score (change 0.0 to a desired threshold if needed)\n",
    "print(f\"Question: {scispace_question}\")\n",
    "print(f\"Answer: {scispace_answer['answer']}\")\n",
    "print(f\"Score: {scispace_answer['score']}\")\n",
    "print(\"----------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz3wjdR7Hvrl"
   },
   "source": [
    " # Trying to identify the Pattern \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "4smDrRZIN0l8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected patterns in 'sci.med' clues:\n",
      "Clue: from lines and shapes to scenes so bright i give digital creations the illusion of light\n",
      "Chemicals: ['from', 'lines', 'and', 'shapes', 'to', 'scenes', 'so', 'bright', 'i', 'give', 'digital', 'creations', 'the', 'illusion', 'of', 'light']\n",
      "Science Terms: ['from', 'lines', 'and', 'shapes', 'to', 'scenes', 'so', 'bright', 'i', 'give', 'digital', 'creations', 'the', 'illusion', 'of', 'light']\n",
      "----------------\n",
      "Clue: where polygons dance and shaders weave my artistry helps the virtual world breathe\n",
      "Chemicals: ['where', 'polygons', 'dance', 'and', 'shaders', 'weave', 'my', 'artistry', 'helps', 'the', 'virtual', 'world', 'breathe']\n",
      "Science Terms: ['where', 'polygons', 'dance', 'and', 'shaders', 'weave', 'my', 'artistry', 'helps', 'the', 'virtual', 'world', 'breathe']\n",
      "----------------\n",
      "Clue: tiny squares a canvas wide building images side by side\n",
      "Chemicals: ['tiny', 'squares', 'a', 'canvas', 'wide', 'building', 'images', 'side', 'by', 'side']\n",
      "Science Terms: ['tiny', 'squares', 'a', 'canvas', 'wide', 'building', 'images', 'side', 'by', 'side']\n",
      "----------------\n",
      "Clue: rgb spells my secret code the building blocks where colors explode\n",
      "Chemicals: ['rgb', 'spells', 'my', 'secret', 'code', 'the', 'building', 'blocks', 'where', 'colors', 'explode']\n",
      "Science Terms: ['rgb', 'spells', 'my', 'secret', 'code', 'the', 'building', 'blocks', 'where', 'colors', 'explode']\n",
      "----------------\n",
      "Detected patterns in 'sci.space' clues:\n",
      "Clue: from lines and shapes to scenes so bright i give digital creations the illusion of light\n",
      "Space Terms: ['from', 'lines', 'and', 'shapes', 'to', 'scenes', 'so', 'bright', 'i', 'give', 'digital', 'creations', 'the', 'illusion', 'of', 'light']\n",
      "Celestial Terms: ['from', 'lines', 'and', 'shapes', 'to', 'scenes', 'so', 'bright', 'i', 'give', 'digital', 'creations', 'the', 'illusion', 'of', 'light']\n",
      "----------------\n",
      "Clue: where polygons dance and shaders weave my artistry helps the virtual world breathe\n",
      "Space Terms: ['where', 'polygons', 'dance', 'and', 'shaders', 'weave', 'my', 'artistry', 'helps', 'the', 'virtual', 'world', 'breathe']\n",
      "Celestial Terms: ['where', 'polygons', 'dance', 'and', 'shaders', 'weave', 'my', 'artistry', 'helps', 'the', 'virtual', 'world', 'breathe']\n",
      "----------------\n",
      "Clue: tiny squares a canvas wide building images side by side\n",
      "Space Terms: ['tiny', 'squares', 'a', 'canvas', 'wide', 'building', 'images', 'side', 'by', 'side']\n",
      "Celestial Terms: ['tiny', 'squares', 'a', 'canvas', 'wide', 'building', 'images', 'side', 'by', 'side']\n",
      "----------------\n",
      "Clue: rgb spells my secret code the building blocks where colors explode\n",
      "Space Terms: ['rgb', 'spells', 'my', 'secret', 'code', 'the', 'building', 'blocks', 'where', 'colors', 'explode']\n",
      "Celestial Terms: ['rgb', 'spells', 'my', 'secret', 'code', 'the', 'building', 'blocks', 'where', 'colors', 'explode']\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "##  Function to detect patterns in clues using regular expressions\n",
    "def detect_patterns(clues_list):\n",
    "    patterns = []\n",
    "    for clue in clues_list:\n",
    "        # Pattern for chemical names (e.g., serotonin, tryptophan)\n",
    "        chemical_pattern = r\"\\b[A-Za-z]+\\b\"\n",
    "        chemicals = re.findall(chemical_pattern, clue)\n",
    "\n",
    "        # Pattern for scientific terms (e.g., neurotransmitter, microbiome)\n",
    "        science_term_pattern = r\"\\b[A-Za-z]+\\b\"\n",
    "        science_terms = re.findall(science_term_pattern, clue)\n",
    "\n",
    "        # Pattern for space-related terms (e.g., rockets, gravity, satellites)\n",
    "        space_term_pattern = r\"\\b[A-Za-z]+\\b\"\n",
    "        space_terms = re.findall(space_term_pattern, clue)\n",
    "\n",
    "        # Pattern for celestial bodies or phenomena (e.g., stars, cosmos)\n",
    "        celestial_pattern = r\"\\b[A-Za-z]+\\b\"\n",
    "        celestial_terms = re.findall(celestial_pattern, clue)\n",
    "\n",
    "        if chemicals or science_terms or space_terms or celestial_terms:\n",
    "            patterns.append(\n",
    "                {\n",
    "                    \"clue\": clue,\n",
    "                    \"chemicals\": chemicals,\n",
    "                    \"science_terms\": science_terms,\n",
    "                    \"space_terms\": space_terms,\n",
    "                    \"celestial_terms\": celestial_terms,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return patterns\n",
    "\n",
    "\n",
    "# Detect patterns in 'sci.med' and 'sci.space' clues\n",
    "scimed_patterns = detect_patterns(scimed_clues_list)\n",
    "scispace_patterns = detect_patterns(scispace_clues_list)\n",
    "\n",
    "print(\"Detected patterns in 'sci.med' clues:\")\n",
    "for pattern in scimed_patterns:\n",
    "    print(f\"Clue: {pattern['clue']}\")\n",
    "    if pattern[\"chemicals\"]:\n",
    "        print(f\"Chemicals: {pattern['chemicals']}\")\n",
    "    if pattern[\"science_terms\"]:\n",
    "        print(f\"Science Terms: {pattern['science_terms']}\")\n",
    "    print(\"----------------\")\n",
    "\n",
    "print(\"Detected patterns in 'sci.space' clues:\")\n",
    "for pattern in scispace_patterns:\n",
    "    print(f\"Clue: {pattern['clue']}\")\n",
    "    if pattern[\"space_terms\"]:\n",
    "        print(f\"Space Terms: {pattern['space_terms']}\")\n",
    "    if pattern[\"celestial_terms\"]:\n",
    "        print(f\"Celestial Terms: {pattern['celestial_terms']}\")\n",
    "    print(\"----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Technical Report: Text Treasure Hunt - The Vectorization Adventure\n",
    "\n",
    "1. Introduction\n",
    "\n",
    "In this report, we embark on the journey of \"The Vectorization Adventure,\" where we delve into the world of text processing and vectorization. Our mission is to decode clues, uncover hidden connections, and collaborate with other teams to reach the ultimate treasure. The adventure involves several stages, each requiring different techniques and approaches to progress towards the final goal.\n",
    "\n",
    "2. Setting Up\n",
    "\n",
    "We start by setting up our environment, ensuring we have all the necessary libraries installed and importing them into our Python environment. The libraries include NLTK, Pandas, Scikit-learn, Gensim, and Spacy. Additionally, we provide an optional section for advanced exploration with Transformers, which requires installation and importation of the Transformers library.\n",
    "\n",
    "3.  Quest Begins - The Initial Clue\n",
    "\n",
    "The first step is to decipher the initial clue provided by our instructor. We carefully analyze the clue to identify words or themes that stand out, which will guide us towards selecting the relevant topic category within the Newsgroup 20 dataset.\n",
    "\n",
    "4. Keyword Quest\n",
    "\n",
    "With our initial clue in hand, we proceed to extract keywords using TF-IDF (Term Frequency-Inverse Document Frequency). We define a function extract_keywords that applies TF-IDF to the selected texts, extracting relevant keywords that may hint at the next topic or text to explore. The extracted keywords help illuminate our path forward.\n",
    "\n",
    "5. Semantic Safari\n",
    "\n",
    "Next, we embark on a semantic analysis journey using word embeddings like Word2Vec or GloVe. We calculate similarities between our extracted keywords and texts in other categories, seeking unexpected connections that may lead us closer to the treasure. \n",
    "Advanced exploration with Transformers is also provided for a deeper semantic understanding.\n",
    "\n",
    "6. Pattern Pursuit\n",
    "\n",
    "In this stage, we search for unusual patterns within the texts using regular expressions. We provide examples of regular expressions to find potential codes or emails within the text data. By examining letter sequences, numbers, or other patterns, we may uncover hidden clues crucial for progressing in the adventure.\n",
    "\n",
    "7. Collaboration and Convergence\n",
    "\n",
    "Teamwork is essential for success in this adventure. We discuss effective communication strategies for sharing findings and combining insights with other teams. Collaboration is key to solving the ultimate puzzle and locating the treasure by converging all the gathered clues.\n",
    "\n",
    "8. Reflection and Report\n",
    "\n",
    "Finally, we reflect on our journey, documenting the methods, techniques, and insights gained at each stage. We provide detailed explanations of the code snippets used and their significance in progressing through the adventure. We discuss the most helpful text processing techniques, the empowering nature of vectorization, surprising discoveries made, and potential real-world applications of our skills.\n",
    "\n",
    "In conclusion, \"The Vectorization Adventure\" is a challenging yet rewarding journey through the realm of text processing and vectorization. By carefully analyzing clues, extracting keywords, exploring semantic meanings, and uncovering patterns, we inch closer to the ultimate treasure. Through collaboration and reflection, we not only solve the adventure but also gain valuable insights and skills applicable in various real-world scenarios.\n",
    "\n",
    "\n",
    "\n",
    "### Reflection\n",
    "\n",
    "Which text processing techniques were most helpful and why?\n",
    "- TF-IDF: Quickly identified the most relevant keywords, helping us zero in on the core themes.\n",
    "- Word Embeddings (Word2Vec): Showed us how words relate to each other, revealing hidden connections and synonyms.\n",
    "\n",
    "How did vectorization empower you to find hidden connections?\n",
    "- Turning words into numbers allowed us to use mathematical tools to spot patterns and similarities that would be hard to see with just our eyes.\n",
    "\n",
    "What was the most surprising part of this adventure?\n",
    "- I was amazed at how regular expressions uncovered a hidden code-like pattern â€“ it felt like cracking a secret message!\n",
    "\n",
    "How could you use these skills for other problems in the real world?\n",
    "- Sentiment Analysis: Understanding the positive or negative \"tone\" of customer reviews or social media posts.\n",
    "- Topic Modeling: Discovering the underlying themes in large collections of news articles or other texts.\n",
    "- Better Search Engines: Creating search tools that find results based on the meaning of words, not just exact matches.\n",
    "\n",
    "Thank you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 2 - Building an AI Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Keywords:\n",
      "Clue 1: andor, artistic, art\n",
      "Clue 2: artistic, art, andor\n",
      "Clue 3: artistic, art, andor\n",
      "Clue 4: artistic, art, andor\n",
      "\n",
      "Word2Vec Keywords:\n",
      "Keyword 1: squares\n",
      "Keyword 2: spells\n",
      "Keyword 3: and\n",
      "Keyword 4: process\n",
      "Keyword 5: essential\n",
      "Keyword 6: the\n",
      "Keyword 7: artistry\n",
      "Keyword 8: helps\n",
      "Keyword 9: shaders\n",
      "Keyword 10: canvas\n",
      "Keyword 11: as\n",
      "Keyword 12: helps\n",
      "Keyword 13: my\n",
      "Keyword 14: wide\n",
      "Keyword 15: range\n",
      "Keyword 16: the\n",
      "Keyword 17: artistry\n",
      "Keyword 18: helps\n",
      "Keyword 19: shaders\n",
      "Keyword 20: canvas\n",
      "Keyword 21: as\n",
      "Keyword 22: helps\n",
      "Keyword 23: my\n",
      "Keyword 24: wide\n",
      "Keyword 25: range\n",
      "Keyword 26: squares\n",
      "Keyword 27: spells\n",
      "Keyword 28: and\n",
      "Keyword 29: process\n",
      "Keyword 30: essential\n",
      "Keyword 31: the\n",
      "Keyword 32: artistry\n",
      "Keyword 33: helps\n",
      "Keyword 34: shaders\n",
      "Keyword 35: canvas\n",
      "Keyword 36: as\n",
      "Keyword 37: helps\n",
      "Keyword 38: my\n",
      "Keyword 39: wide\n",
      "Keyword 40: range\n",
      "Keyword 41: squares\n",
      "Keyword 42: spells\n",
      "Keyword 43: and\n",
      "Keyword 44: process\n",
      "Keyword 45: essential\n",
      "Keyword 46: the\n",
      "Keyword 47: artistry\n",
      "Keyword 48: helps\n",
      "Keyword 49: shaders\n",
      "Keyword 50: canvas\n",
      "Keyword 51: as\n",
      "Keyword 52: helps\n",
      "Keyword 53: my\n",
      "Keyword 54: wide\n",
      "Keyword 55: range\n",
      "Keyword 56: squares\n",
      "Keyword 57: spells\n",
      "Keyword 58: and\n",
      "Keyword 59: process\n",
      "Keyword 60: essential\n",
      "\n",
      "Detected Patterns:\n",
      "{'clue': 'digital art encompasses a range of artistic work and practices that use digital technology as an essential part of the creative andor presentation process', 'chemicals': ['digital', 'art', 'encompasses', 'a', 'range', 'of', 'artistic', 'work', 'and', 'practices', 'that', 'use', 'digital', 'technology', 'as', 'an', 'essential', 'part', 'of', 'the', 'creative', 'andor', 'presentation', 'process'], 'science_terms': ['digital', 'art', 'encompasses', 'a', 'range', 'of', 'artistic', 'work', 'and', 'practices', 'that', 'use', 'digital', 'technology', 'as', 'an', 'essential', 'part', 'of', 'the', 'creative', 'andor', 'presentation', 'process'], 'space_terms': ['digital', 'art', 'encompasses', 'a', 'range', 'of', 'artistic', 'work', 'and', 'practices', 'that', 'use', 'digital', 'technology', 'as', 'an', 'essential', 'part', 'of', 'the', 'creative', 'andor', 'presentation', 'process'], 'celestial_terms': ['digital', 'art', 'encompasses', 'a', 'range', 'of', 'artistic', 'work', 'and', 'practices', 'that', 'use', 'digital', 'technology', 'as', 'an', 'essential', 'part', 'of', 'the', 'creative', 'andor', 'presentation', 'process']}\n",
      "{'clue': 'where polygons dance and shaders weave my artistry helps the virtual world breathe', 'chemicals': ['where', 'polygons', 'dance', 'and', 'shaders', 'weave', 'my', 'artistry', 'helps', 'the', 'virtual', 'world', 'breathe'], 'science_terms': ['where', 'polygons', 'dance', 'and', 'shaders', 'weave', 'my', 'artistry', 'helps', 'the', 'virtual', 'world', 'breathe'], 'space_terms': ['where', 'polygons', 'dance', 'and', 'shaders', 'weave', 'my', 'artistry', 'helps', 'the', 'virtual', 'world', 'breathe'], 'celestial_terms': ['where', 'polygons', 'dance', 'and', 'shaders', 'weave', 'my', 'artistry', 'helps', 'the', 'virtual', 'world', 'breathe']}\n",
      "{'clue': 'tiny squares a canvas wide building images side by side', 'chemicals': ['tiny', 'squares', 'a', 'canvas', 'wide', 'building', 'images', 'side', 'by', 'side'], 'science_terms': ['tiny', 'squares', 'a', 'canvas', 'wide', 'building', 'images', 'side', 'by', 'side'], 'space_terms': ['tiny', 'squares', 'a', 'canvas', 'wide', 'building', 'images', 'side', 'by', 'side'], 'celestial_terms': ['tiny', 'squares', 'a', 'canvas', 'wide', 'building', 'images', 'side', 'by', 'side']}\n",
      "{'clue': 'rgb spells my secret code the building blocks where colors explode', 'chemicals': ['rgb', 'spells', 'my', 'secret', 'code', 'the', 'building', 'blocks', 'where', 'colors', 'explode'], 'science_terms': ['rgb', 'spells', 'my', 'secret', 'code', 'the', 'building', 'blocks', 'where', 'colors', 'explode'], 'space_terms': ['rgb', 'spells', 'my', 'secret', 'code', 'the', 'building', 'blocks', 'where', 'colors', 'explode'], 'celestial_terms': ['rgb', 'spells', 'my', 'secret', 'code', 'the', 'building', 'blocks', 'where', 'colors', 'explode']}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "\n",
    "# Function to extract keywords using Word2Vec\n",
    "def extract_keywords_word2vec(clues_list, keywords, n=5):\n",
    "    # Tokenize the clues\n",
    "    tokenized_clues = [clue.split() for clue in clues_list]\n",
    "\n",
    "    # Create a Word2Vec model\n",
    "    word2vec = Word2Vec(tokenized_clues, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "    related_keywords = []\n",
    "    for keyword_list in keywords:\n",
    "        for keyword in keyword_list:\n",
    "            try:\n",
    "                # Get the most similar keywords for each input keyword\n",
    "                related = word2vec.wv.most_similar(positive=[keyword], topn=n)\n",
    "                related_keywords.extend([w[0] for w in related])\n",
    "            except KeyError:\n",
    "                # Skip keywords not in the vocabulary\n",
    "                pass\n",
    "\n",
    "    return related_keywords\n",
    "\n",
    "# Example usage\n",
    "text_data = [\n",
    "    \"Digital art encompasses a range of artistic work and practices that use digital technology as an essential part of the creative and/or presentation process.\",\n",
    "    \"Space exploration is the investigation and discovery of outer space, including celestial bodies and phenomena.\",\n",
    "    \"The human brain is a complex organ responsible for various cognitive functions and behaviors.\",\n",
    "]\n",
    "\n",
    "clues = [\n",
    "    \"Digital art encompasses a range of artistic work and practices that use digital technology as an essential part of the creative and/or presentation process.\",\n",
    "    \"Where polygons dance and shaders weave, my artistry helps the virtual world breathe.\",\n",
    "    \"Tiny squares, a canvas wide, building images side by side.\",\n",
    "    \"RGB spells my secret code, the building blocks where colors explode.\",\n",
    "]\n",
    "\n",
    "# Clean text data and clues\n",
    "cleaned_text_data = [clean_text(text) for text in text_data]\n",
    "cleaned_clues = [clean_text(clue) for clue in clues]\n",
    "\n",
    "# Extract keywords using TF-IDF\n",
    "tfidf_keywords = extract_keywords_tfidf(cleaned_text_data, cleaned_clues)\n",
    "\n",
    "# Extract keywords using Word2Vec\n",
    "word2vec_keywords = extract_keywords_word2vec(cleaned_clues, tfidf_keywords)\n",
    "\n",
    "# Detect patterns in clues\n",
    "patterns = detect_patterns(cleaned_clues)\n",
    "\n",
    "# Print the results\n",
    "print(\"TF-IDF Keywords:\")\n",
    "for i, clue_keywords in enumerate(tfidf_keywords):\n",
    "    print(f\"Clue {i+1}: {', '.join(clue_keywords)}\")\n",
    "\n",
    "print(\"\\nWord2Vec Keywords:\")\n",
    "for i, keyword in enumerate(word2vec_keywords):\n",
    "    print(f\"Keyword {i+1}: {keyword}\")\n",
    "\n",
    "print(\"\\nDetected Patterns:\")\n",
    "for pattern in patterns:\n",
    "    print(pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using a Trans: Take 2\n",
    "#### gemime -slow, GPT - trumps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Keywords: [['work', 'use', 'the', 'that', 'technology'], ['the', 'since', 'significantly', 'has', 'evolved'], ['well', 'traditional', 'such', 'sculpture', 'photography']]\n",
      "Word2Vec Keywords: ['like', 'art', 'process.', 'Modern', 'part', 'of', 'art,', 'installations.', 'essential', 'and', 'include', 'media', 'photography', 'art,', 'a', 'include', 'painting', 'of', 'creative', 'practices', 'and/or', 'sculpture,', 'and', 'traditional', 'Modern', 'include', 'media', 'photography', 'art,', 'a', 'art', 'interactive', 'media', 'photography', 'an', 'interactive', 'the', 'painting', '1960s.', 'range', '1960s.', 'painting', 'practices', 'as', 'traditional', 'practices', 'traditional', 'painting', 'range', 'as', 'encompasses', 'presentation', 'of', 'both', 'Digital', 'evolved', 'essential', 'as', 'both', 'part', 'painting', 'a', 'can', 'an', 'art', 'of', 'the', 'media', 'since', 'can']\n",
      "Patterns in Clues: [{'clue': 'Digital art encompasses a range of artistic work and practices that use digital technology as an essential part of the creative and/or presentation process.', 'chemicals': ['Digital', 'art', 'encompasses', 'a', 'range', 'of', 'artistic', 'work', 'and', 'practices', 'that', 'use', 'digital', 'technology', 'as', 'an', 'essential', 'part', 'of', 'the', 'creative', 'and', 'or', 'presentation', 'process'], 'science_terms': [], 'space_terms': [], 'celestial_terms': []}, {'clue': 'Digital art has evolved significantly since the 1960s.', 'chemicals': ['Digital', 'art', 'has', 'evolved', 'significantly', 'since', 'the', 's'], 'science_terms': [], 'space_terms': [], 'celestial_terms': []}, {'clue': 'Modern digital art can include both traditional forms of art, such as painting and sculpture, as well as new media forms like digital photography and interactive installations.', 'chemicals': ['Modern', 'digital', 'art', 'can', 'include', 'both', 'traditional', 'forms', 'of', 'art', 'such', 'as', 'painting', 'and', 'sculpture', 'as', 'well', 'as', 'new', 'media', 'forms', 'like', 'digital', 'photography', 'and', 'interactive', 'installations'], 'science_terms': [], 'space_terms': [], 'celestial_terms': []}]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())  # Remove punctuation and convert to lowercase\n",
    "    return text\n",
    "\n",
    "## Function to detect patterns in clues using regular expressions\n",
    "def detect_patterns(clues_list):\n",
    "    patterns = []\n",
    "    for clue in clues_list:\n",
    "        # Improved patterns with some examples\n",
    "        chemical_pattern = r\"[A-Z][a-z]*[a-z0-9]*|[A-Z][0-9][a-z]*\"  # Example: Serotonin, Dopamine\n",
    "        science_term_pattern = r\"\\b(neurotransmitter|microbiome|receptor|enzyme)\\b\"  # Example list\n",
    "        space_term_pattern = r\"\\b(rocket|satellite|astronaut|gravity|planet)\\b\"  # Example list\n",
    "        celestial_pattern = r\"\\b(star|galaxy|black hole|constellation)\\b\"  # Example list\n",
    "\n",
    "        chemicals = re.findall(chemical_pattern, clue, flags=re.IGNORECASE)\n",
    "        science_terms = re.findall(science_term_pattern, clue, flags=re.IGNORECASE)\n",
    "        space_terms = re.findall(space_term_pattern, clue, flags=re.IGNORECASE)\n",
    "        celestial_terms = re.findall(celestial_pattern, clue, flags=re.IGNORECASE)\n",
    "\n",
    "        if chemicals or science_terms or space_terms or celestial_terms:\n",
    "            patterns.append({\n",
    "                \"clue\": clue,\n",
    "                \"chemicals\": chemicals,\n",
    "                \"science_terms\": science_terms,\n",
    "                \"space_terms\": space_terms,\n",
    "                \"celestial_terms\": celestial_terms,\n",
    "            })\n",
    "\n",
    "    return patterns\n",
    "\n",
    "\n",
    "# Function to extract keywords using TF-IDF\n",
    "def extract_keywords_tfidf(text_data, clues):\n",
    "    cleaned_text_data = [clean_text(text) for text in text_data]\n",
    "    cleaned_clues = [clean_text(clue) for clue in clues]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
    "    tfidf_matrix = vectorizer.fit_transform(cleaned_text_data)\n",
    "    clue_vectors = vectorizer.transform(cleaned_clues)\n",
    "\n",
    "    tfidf_keywords = []\n",
    "    for clue_vector, clue in zip(clue_vectors, cleaned_clues):\n",
    "        # Get top n keywords with highest TF-IDF scores for each clue\n",
    "        top_n_indices = clue_vector.indices[:5]  # Adjust number of keywords (n) as needed\n",
    "        top_keywords = [vectorizer.get_feature_names_out()[i] for i in top_n_indices]\n",
    "        tfidf_keywords.append(top_keywords)\n",
    "\n",
    "    return tfidf_keywords\n",
    "\n",
    "\n",
    "# Function to extract keywords using Word2Vec\n",
    "def extract_keywords_word2vec(clues_list, keywords, n=5):\n",
    "    # Tokenize the clues\n",
    "    tokenized_clues = [clue.split() for clue in clues_list]\n",
    "\n",
    "    # Create a Word2Vec model\n",
    "    try:\n",
    "        word2vec = Word2Vec(tokenized_clues, vector_size=100, window=5, min_count=1, sg=1)\n",
    "    except:\n",
    "        print(\"Error creating Word2Vec model. Skipping Word2Vec keyword extraction.\")\n",
    "        return []\n",
    "\n",
    "    related_keywords = []\n",
    "    for keyword_list in keywords:\n",
    "        for keyword in keyword_list:\n",
    "            try:\n",
    "                # Get the most similar keywords for each input keyword\n",
    "                related = word2vec.wv.most_similar(positive=[keyword], topn=n)\n",
    "                related_keywords.extend([w[0] for w in related])\n",
    "            except KeyError:\n",
    "                # Skip keywords not in the vocabulary\n",
    "                pass\n",
    "\n",
    "    return related_keywords\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text_data = [\n",
    "    \"Digital art encompasses a range of artistic work and practices that use digital technology as an essential part of the creative and/or presentation process.\",\n",
    "    \"The digital art field has evolved significantly since the 1960s.\",\n",
    "    \"Modern digital art can include both traditional forms of art, such as painting and sculpture, as well as new media forms like digital photography and interactive installations.\"\n",
    "]\n",
    "\n",
    "clues = [\n",
    "    \"Digital art encompasses a range of artistic work and practices that use digital technology as an essential part of the creative and/or presentation process.\",\n",
    "    \"Digital art has evolved significantly since the 1960s.\",\n",
    "    \"Modern digital art can include both traditional forms of art, such as painting and sculpture, as well as new media forms like digital photography and interactive installations.\"\n",
    "]\n",
    "\n",
    "# Extract keywords using TF-IDF\n",
    "tfidf_keywords = extract_keywords_tfidf(text_data, clues)\n",
    "print(\"TF-IDF Keywords:\", tfidf_keywords)\n",
    "\n",
    "# Extract keywords using Word2Vec\n",
    "word2vec_keywords = extract_keywords_word2vec(clues, tfidf_keywords)\n",
    "print(\"Word2Vec Keywords:\", word2vec_keywords)\n",
    "\n",
    "# Detect patterns in clues\n",
    "patterns = detect_patterns(clues)\n",
    "print(\"Patterns in Clues:\", patterns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
